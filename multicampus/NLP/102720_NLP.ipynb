{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow, nltk, konlpy\n",
    "# nltk.download()\n",
    "# https://www.oracle.com/java/technologies/javase-jdk15-downloads.html\n",
    "# https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype\n",
    "import nltk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰? 자연어 처리 대상(단위: 단어, 문장, 문단...)\n",
    "# 토큰: 문법적으로 더 이상 나눌 수 없는 요소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"After preventing President Obama from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"\n",
    "short_text = \"Hello, I'm Yoon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', \"'m\", 'Yoon', '.']\n",
      "Hello , I 'm Yoon .\n",
      "\n",
      "['Hello', ',', 'I', \"'\", 'm', 'Yoon', '.']\n",
      "Hello , I ' m Yoon .\n",
      "\n",
      "['hello', \"i'm\", 'yoon']\n",
      "hello i'm yoon\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(short_text))\n",
    "print(*word_tokenize(short_text), end=\"\\n\\n\")\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(short_text))\n",
    "print(*WordPunctTokenizer().tokenize(short_text), end=\"\\n\\n\")\n",
    "\n",
    "print(text_to_word_sequence(short_text))\n",
    "print(*text_to_word_sequence(short_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr', '.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'preventing', 'president', 'obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', 'republicans', 'moved', 'swiftly', 'to', 'deliver', 'president', 'trump', 'a', 'victory', 'days', 'before', 'the', 'election', 'both', 'presidential', 'candidates', 'campaigned', 'in', 'pennsylvania', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'mr', 'trump', 'mocked', 'kamala', 'harris']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['Hello, Mr. Kim.']\n",
      "3 ['Hello, My name is Yoon.', 'I have a B.S.', 'in Computer Science.']\n",
      "2 ['Hello, My name is Yoon.', 'I have a Ph.D. in Computer Science.']\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스\n",
    "# 문장 토큰화 : 마침표 기준\n",
    "\n",
    "text = \"Hello, Mr. Kim.\"\n",
    "print(len(sent_tokenize(text)), sent_tokenize(text))\n",
    "\n",
    "text = \"Hello, My name is Yoon. I have a B.S. in Computer Science.\"\n",
    "print(len(sent_tokenize(text)), sent_tokenize(text))\n",
    "\n",
    "text = \"Hello, My name is Yoon. I have a Ph.D. in Computer Science.\"\n",
    "print(len(sent_tokenize(text)), sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 형태소: 자립형태소, 의존형태소(조사)\n",
    "# 길동이(자립)가(의존) 딥러닝(자립)을(의존) 합니다\n",
    "\n",
    "# 2) 띄어쓰기\n",
    "# 지금 이 문장을 해석하는 데 어려움이 있나요?\n",
    "# 지금이문장을해석하는데어려움이있나요?\n",
    "\n",
    "# 3) 품사에 따라 단어의 의미가 달라짐\n",
    "# mine: 지뢰, 광물을 캐다\n",
    "# 못: 품사 태깅을 미리 수행하여, 특정 단어가 사용되기 앞서 어떤 품사로 사용되는지 구분(품사태깅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " (',', ','),\n",
       " ('My', 'NNP'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Yoon', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Computer', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "a = word_tokenize(text)\n",
    "pos_tag(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0\n",
    "# https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 : 형태소 단위로 토큰화를 한다는 의미\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '공부', '하고', '있는', '우리', '들', ',', '수료', '후', '에는', '취업', '에', '꼭', '성공해요']\n",
      "['공부', '우리', '수료', '후', '취업', '꼭']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "text = \"열심히 공부하고 있는 우리들, 수료 후에는 취업에 꼭 성공해요\"\n",
    "print(okt.morphs(text))\n",
    "print(okt.nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공부', '우리', '수료', '후', '취업', '성공']\n"
     ]
    }
   ],
   "source": [
    "km = Kkma()\n",
    "print(km.nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 : 코퍼스(특정 도메인 단어 집합)로부터 노이즈 데이터 제거\n",
    "# ex)인공지능/법률 코퍼스(당뇨병)\n",
    "# 정규화 : 동의어들 -> 같은 단어로 통합\n",
    "# 불용어 제거\n",
    "# 빈도수가 낮은 단어 제거\n",
    "# 길이가 짧은 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were\n",
      "have\n",
      "die\n",
      "watch\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출: 해당 단어의 품사 정보에 따라 표제어를 추출\n",
    "print(wnl.lemmatize(\"were\"))\n",
    "print(wnl.lemmatize(\"has\", 'v'))\n",
    "print(wnl.lemmatize(\"dies\", 'v'))\n",
    "print(wnl.lemmatize(\"watched\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간(stem): 활용어가 활용할 때 변하지 않는 부분 (먹-다, 먹-니, 먹-고, 보-다, 보-니, 보-고...)\n",
    "# 어미(ending): 용언 및 서술격 조사가 활용하여 변하는 부분 (점잖-다, 점잖으-며, 점잖-고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'going', 'doing', 'love']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [wnl.lemmatize(w, 'n') for w in [\"lives\", \"going\", \"doing\", \"love\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'My', 'name', 'is', 'Yoon', '.', 'I', 'have', 'a', 'Ph.D.', 'in', 'Computer', 'Science', '.']\n",
      "['hello', ',', 'My', 'name', 'is', 'yoon', '.', 'I', 'have', 'a', 'ph.d.', 'in', 'comput', 'scienc', '.']\n",
      "['hello', ',', 'my', 'nam', 'is', 'yoon', '.', 'i', 'hav', 'a', 'ph.d.', 'in', 'comput', 'sci', '.']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# 포터 알고리즘에 의한 어간 추출\n",
    "print([ps.stem(w) for w in words])\n",
    "\n",
    "# 랭커스터 알고리즘에 의한 어간 추출\n",
    "print([ls.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk 불용어\n",
    "# 불용어(stopwords): 자주 등장하지만 분석을 하는 것에 있어서 큰 도움이 되지 않는 단어(the, a, an, in...)\n",
    "# http://ranks.nl/stopwords/korean\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 포함: ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거: ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Family is not an important thing. It's everything.\"\n",
    "sw = set(stopwords.words('english')) # 영어 불용어 사전\n",
    "wt = word_tokenize(text)\n",
    "\n",
    "print(\"불용어 포함: \" + str(word_tokenize(text)))\n",
    "print(\"불용어 제거: \" + str([w for w in wt if w not in sw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['텐서플로우와', '머신러닝으로', '자연어', '처리를', '하거든', '.', '머신러닝으로만', '자연어', '처리를', '하면', '안돼', '.']\n",
      "['텐서플로우와', '머신러닝으로', '자연어', '처리를', '.', '머신러닝으로만', '자연어', '처리를', '안돼', '.']\n"
     ]
    }
   ],
   "source": [
    "data = \"텐서플로우와 머신러닝으로 자연어 처리를 하거든. 머신러닝으로만 자연어 처리를 하면 안돼.\"\n",
    "sw = \"어쨌든 아무거나 같다 비슷하다 하면 하거든 으로\".split(\" \")\n",
    "\n",
    "wt = word_tokenize(data)\n",
    "print(wt)\n",
    "print([w for w in wt if w not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['010', '1234', '5678', '28']\n"
     ]
    }
   ],
   "source": [
    "# 추출\n",
    "text = \"\"\"\n",
    "전화번호: 010-1234-5678\n",
    "주소 : 서울시 강남구\n",
    "나이 : 28\n",
    "\"\"\"\n",
    "print(re.findall(\"\\d+\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국 대한민국 대한민국 대한민국 대한민국 대한민국'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 치환\n",
    "text = \"대한민국 한국 코리아 korea 남한 고려\"\n",
    "re.sub(\"한국|코리아|korea|남한|고려\", \"대한민국\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her impact could be felt right away.',\n",
       " 'There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania.',\n",
       " 'Both concern the date by which absentee ballots may be accepted.',\n",
       " 'With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right.',\n",
       " 'It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['impact', 'could', 'felt', 'right', 'away'], ['major', 'election', 'disputes', 'awaiting', 'immediate', 'action', 'supreme', 'court', 'battleground', 'states', 'north', 'carolina', 'pennsylvania'], ['concern', 'date', 'absentee', 'ballots', 'may', 'accepted'], ['justice', 'barrett', 'elevation', 'place', 'justice', 'ginsburg', 'liberal', 'icon', 'court', 'expected', 'tilt', 'decisively', 'right'], ['gaining', 'conservative', 'could', 'sway', 'cases', 'every', 'area', 'american', 'life', 'including', 'abortion', 'rights', 'gay', 'rights', 'business', 'regulation', 'environment']]\n",
      "{'impact': 1, 'could': 2, 'felt': 1, 'right': 2, 'away': 1, 'major': 1, 'election': 1, 'disputes': 1, 'awaiting': 1, 'immediate': 1, 'action': 1, 'supreme': 1, 'court': 2, 'battleground': 1, 'states': 1, 'north': 1, 'carolina': 1, 'pennsylvania': 1, 'concern': 1, 'date': 1, 'absentee': 1, 'ballots': 1, 'may': 1, 'accepted': 1, 'justice': 2, 'barrett': 1, 'elevation': 1, 'place': 1, 'ginsburg': 1, 'liberal': 1, 'icon': 1, 'expected': 1, 'tilt': 1, 'decisively': 1, 'gaining': 1, 'conservative': 1, 'sway': 1, 'cases': 1, 'every': 1, 'area': 1, 'american': 1, 'life': 1, 'including': 1, 'abortion': 1, 'rights': 2, 'gay': 1, 'business': 1, 'regulation': 1, 'environment': 1}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 클리닝, 단어 토큰화\n",
    "# 문장 -> 단어 -> 소문자화(단어의 수 줄이기 ex. How, how를 하나의 단어로 인식) -> 불용어 제거 -> 짧은 길이의 단어 제거...\n",
    "text = \"Her impact could be felt right away. There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania. Both concern the date by which absentee ballots may be accepted. With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right. It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.\"\n",
    "text = sent_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "vocab = {}\n",
    "sentences= []\n",
    "\n",
    "for sent in text:\n",
    "    wt = word_tokenize(sent)\n",
    "    res = []\n",
    "    for word in wt:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                res.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "    sentences.append(res)\n",
    "\n",
    "print(sentences)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[['barber', 'person'], \n",
    "           ['barber', 'good', 'person'], \n",
    "           ['barber', 'huge', 'person'], \n",
    "           ['knew', 'secret'], \n",
    "           ['secret', 'kept', 'huge', 'secret'], \n",
    "           ['huge', 'secret'], \n",
    "           ['barber', 'kept', 'word'], \n",
    "           ['barber', 'kept', 'word'], \n",
    "           ['barber', 'kept', 'secret'], \n",
    "           ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], \n",
    "           ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 코퍼스에 등장하는 단어들에 대한 빈도수를 기준으로 단어 집합 생성\n",
    "# 빈도수가 높은 ~ 낮은 단어 순으로 번호를 부여\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "\n",
    "# 단어와 인덱스\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# 단어별 빈도수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어별 인덱스로 변환\n",
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = 5 # 가장 빈도수가 높은 5개 단어만 추출\n",
    "tokenizer = Tokenizer(num_words=vs + 1)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
